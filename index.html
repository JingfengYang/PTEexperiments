<!-- Maintainer: Xingchi Li, https://career.lixingchi.com -->
<!DOCTYPE html>
<html lang="en-us">

<head>
    <meta charset="UTF-8">
    <title>Pte: Predictive text embedding through large-scale heterogeneous text networks.</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
    <script type="text/javascript" , src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.0/jquery.js"></script>
    <script type="text/javascript" , src="https://d3js.org/d3.v5.min.js"></script>
    <script type="text/javascript" , src="https://d3js.org/d3-dsv.v1.min.js"></script>
    <script type="text/javascript" , src="https://d3js.org/d3-fetch.v1.min.js"></script>
    <script type="text/javascript" , src="https://d3js.org/d3-color.v1.min.js"></script>
    <script type="text/javascript" , src="https://d3js.org/d3-interpolate.v1.min.js"></script>
    <script type="text/javascript" , src="https://d3js.org/d3-scale-chromatic.v1.min.js"></script>
    <script type="text/javascript" , src="https://unpkg.com/topojson@3"></script>
    <script type="text/javascript" , src="javascripts/d3-tip.min.js"></script>
    <script type="text/x-mathjax-config">
	    MathJax.Hub.Config({
	      tex2jax: {
	        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
	        inlineMath: [['$','$']]
	      }
	    });
	  </script>
	  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

</head>

<body>
    <section class="page-header">
        <h1 class="project-name">Pte: Predictive text embedding through large-scale heterogeneous text networks.</h1>
        <h2 class="project-tagline"></h2>
        <a href="https://github.com/nathanxli/PTEexperiments" class="btn">View on GitHub</a>
    </section>

    <section class="main-content">
        <h3>
            <a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Introduction
        </h3>
        <h4>
            <a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Motivation
        </h4>
        <p>
            The text contains numerous effective and meaningful information for people. How to learn the meaning of words is a critical prerequisite for many machine learning tasks. Traditionally, people represent words independently to each other and represent each
            document as a "bag of words". However, the sparsity, polysemy, and synonymy of words and documents are commonly ignored during training.
        </p>
        <p>
            One way to deal with it is by representing words and documents in low-dimensional spaces[1]. By the unsupervised learning process, the similar words and documents are embedded closely to each other. For example, Mikilov et al. proposed Skip-gram using
            the embedding of the target words to predict the embedding of individual context word in a local window. [1]. Compared with other classical approaches that utilize the distributional similarity of word contest, this text embedding approach
            is more efficient and easier to deploy[2].
        </p>
        <p>
            However, compared with deep learning approaches such as convolutional neural networks (CNNs), the precision of text embedding method would be lower. This is because a method like CNNs leverage labeled information thus they can learn the representation
            of the data better. In other words, the unsupervised method would be generalizable for a different task but have weaker predictive power for a particular task[2]. However, compared with text embedding, CNNs training also has a drawback. It
            requires numerous parameters which is not only time consuming but also difficult to train an ideal model.
        </p>
        <p>
            To sum up, text embedding methods are more efficient while deep learning neural networks are more predictive. How about using both supervised and unsupervised labels at the same time?
        </p>
        <h4>
            <a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Project scope
        </h4>
        <p>
            In this project, we deploy Predictive Text Embedding (PTE), a semi-supervised representation learning method for text data to fill the gap between supervised and unsupervised labels. By learning from limited labeled examples and a large number of unlabelled
            examples, we obtain an effective low dimensional representation, which is the information between words and words, words and labels and words and documents.
        </p>
        <p>
            We build two kinds of training data: one has a network encodes different levels of co-occurrence information between words and words while other does not. Then, we conduct experiments with sentiment data sets MR, short document data sets DBLP and long
            document data sets 20NG. Experimental results show that the data which has the word and word information outperform the data without this embedding information in various text classification tasks.
        </p>
        <h3>
            <a id="designer-templates" class="anchor" href="#designer-templates" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Methods
        </h3>
        <p>
            In semi-supervised sentence classification, to leverage both annotated data and unannotated text data, we first build a heterogeneous graph whose edge consists of word-sentence, word-word, and word-label. Then we learn the representations of each vertex
            in the graph. Finally, we could use them as input of the text classification task. This framework is proposed by <a href="https://arxiv.org/abs/1508.00200">Tang et al.</a>. There are several methods we would like to try in the
            second step. The first is to use <a href="https://arxiv.org/abs/1508.00200">PTE</a> training process. The second is to use <a href="https://arxiv.org/abs/1609.02907">Graph Convolutional Networks ( GCN )</a>. <a href="https://arxiv.org/abs/1809.05679">Yao et al.</a> has tried GCN in the text classification task, but they did not include existing labels as vertices in the graph. We argue that label vertices are crucial, because it can introduce supervised information when training word embeddings. Because
            of the heterogeneity, we would like to try techniques in Heterogeneous Graph Neural Network. We plan to use three datasets to test our model: <a href="http://www.cs.cornell.edu/people/pabo/movie-review-data/">one movie review dataset</a> and
            two sentiment classification datasets: <a href="http://www.cs.jhu.edu/~mdredze/datasets/sentiment/">Multi-Domain Sentiment Dataset (version 2.0)</a>, <a href="http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/">Twitter Sentiment Analysis Training Corpus (Dataset)</a>.
        </p>
        <h3>
            <a id="creating-pages-manually" class="anchor" href="#creating-pages-manually" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Results
        </h3>
            <h4>
                <a class="anchor" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Results for DBLP dataset
            </h4>
            <p id="word-label">
                We will be able to determine the approach with the best performance using the three datasets and the contribution of labeled and unlabeled data in semi-supervised learning to the final result, as well as the efficiency and the parameter used. Finally, we also present document visualizations to vividly illustrate our result.

                As seen in the following graph, we have separated the nodes into two classifications: text and label. There are six labels in the embedding, connecting the texts using yellow-ish lines. Among the $1.25$ million texts, we picked around $100$ texts to show what we have done, the link between which are colored steelblue.
            </p>
            <h4>
                <a id="creating-pages-manually" class="anchor" href="#creating-pages-manually" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Results for MR dataset
            </h4>
            <p id="word-label-mr">
                There are $600$ thousand words and only two labels in the MR dataset. We are able to pick $100$ of them to make a clear relationship out of them. Similar to DBLP dataset, we used yellow line to draw the word-label relationship and line in steelblue to draw the word-word relationship line.
            </p>
        <h3 id="graph-convolutional-network-trial">Graph Convolutional Network Trial</h3>
            <h4 id="procedure">Procedure</h4>
            <ul>
                <li>
                    Remove stop word: we remove the stop words and low-frequency words (conducted in the PTE preprocessing).
                </li>
                <li>
                    Build graph: we build graphs by the network construction results in PTE model because in the PTE graph construction processing, we have three graphs: (1) word-word network (2) word-document network (3) word-label network; we can use the first two graphs to build a new graph where each node can be a word/document and an edge indicates the co-currence. To normalize it, we use TF-IDF instead. (For computing convenience, we directly use the text data.)
                </li>
                <li>
                    Learn graph convolutional network: once a graph is built, we use the classical GCN learning method, where we only have two layers.
                </li>
            </ul>
            <h4 id="evaluation">Evaluation</h4>
            <p>
                We compare PTE and GCN wby MR dataset with label ratio equal to 1. As we can see, GCN has a relatively higher F1 score. The reason is that GCN usually learns a better graph-structure representation than the embedding vector by PTE.
                <table>
                <thead>
                <tr>
                <th></th>
                <th style="text-align:center">PTE</th>
                <th style="text-align:right">GCN</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                <td>F1-micro/F1-macro</td>
                <td style="text-align:center">0.65</td>
                <td style="text-align:right">0.68</td>
                </tr>
                </tbody>
                </table>
            </p>
        <h3>
            <a id="creating-pages-manually" class="anchor" href="#creating-pages-manually" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Line charts comparison of Linear Regression, CNN and Support Vector Machine
        </h3>
        <p id="mr-line">
            <ol>
                <li>Different classification results:<br/>
                As we can see in the result, SVM mainly beats LR and CNN in the metric of F1 micro/macro socres under this text classification task. The reason may be that SVM can better caputure the embedding space features and make a good prediction results.
                </li>
                <li>Different label ratio results:<br/>
                The result shows that with the percentage of the labeled data increasing (i.e., 0.125, 0.25, 0.5, 1), the F1 score increases accordingly. It is easy to interprate this finding since with more labeled data, we can learn a more precise model from the objective function. More labeled data indicates a bigger feature space that can be learnted.
                </li>
                <li>Different supervised learning setting: Semi-supervised advantage:<br/>
                In the experiment, we also test the built graph under two kinds of dataset: (1) graph with only labeled data (2) graph with labeled and unlabeled data. As is shown in the result, the added unlabeled data can improve the F1 score. This is because when the unlabeled data is added in the graph construction process, a better word embedding is generated, leading to a higher F1 score of labeled+unlabel data than that of labeled data.
                </li>
            </ol>
            <table style="width:100%">
              <tr>
                <th>F-MACRO</th>
                <th>F-MICRO</th>
              </tr>
              <tr>
                <td id="mult-line-a-mr-1"></td>
                <td id="mult-line-i-mr-1"></td>
              </tr>
              <tr>
                <td id="mult-line-a-mr-2"></td>
                <td id="mult-line-i-mr-2"></td>
              </tr>
              <tr>
                <td id="mult-line-a-mr-3"></td>
                <td id="mult-line-i-mr-3"></td>
              </tr>
              <tr>
                <td id="mult-line-a-dblp-1"></td>
                <td id="mult-line-i-dblp-1"></td>
              </tr>
              <tr>
                <td id="mult-line-a-dblp-2"></td>
                <td id="mult-line-i-dblp-2"></td>
              </tr>
              <tr>
                <td id="mult-line-a-dblp-3"></td>
                <td id="mult-line-i-dblp-3"></td>
              </tr>
              <tr>
                <td id="mult-line-a-20ng-1"></td>
                <td id="mult-line-i-20ng-1"></td>
              </tr>
              <tr>
                <td id="mult-line-a-20ng-2"></td>
                <td id="mult-line-i-20ng-2"></td>
              </tr>
              <tr>
                <td id="mult-line-a-20ng-3"></td>
                <td id="mult-line-i-20ng-3"></td>
              </tr>
            </table>
        </p>
        <h3>
            <a id="creating-pages-manually" class="anchor" href="#creating-pages-manually" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Scatter Plot of MR
        </h3>
        <p id="scatter-mr">
            There are two labels in the MR dataset. We used <code>sklearn.manifold.TSNE</code> by extracting the two main components from the dataset containing $0.6$ million texts and labels, thus better illustrating the idea of the embedding.
        </p>
        <h3>
            <a id="creating-pages-manually" class="anchor" href="#creating-pages-manually" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Scatter Plot of DBLP
        </h3>
        <p id="scatter-dblp">
            There are six labels in the DBLP dataset. We used <code>sklearn.manifold.TSNE</code> by extracting the two main components from the dataset containing $1.25$ million texts and labels. To better illustrate the idea of the embedding, we added the functionality to choose among the labels to show. The label input accepts number inputs including number $1$ to $6$ and the graph is generated based on the input labels which by default is $123456$, i.e. including all the labels. (Please have patience as the graph is generated real-time, thus depending on your network and processor.)<br/>
            <label for="labelValue"
                 style="display: inline-block; width: 240px; text-align: right">
                 label = <span id="labelValue-value"></span>
            </label>
            <input type="text" value="123456" id="labelValue">
        </p>
        <h3>
            <a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Discussion
        </h3>
        <p>
            PTE relates the unlabeled training data directly to the specific task, outperforming methods that train word/semantic embedding separately. Thus, we expect that utilizing recent advancements in graph-based neural network structure and traning method like
            proposed in <a href="https://arxiv.org/abs/1609.02907">Semi-Supervised Classification with Graph Convolutional Networks</a> can further enhance PTE method. We also expect that techniques in Heterogeneous Graph Neural Network like
            attention mechanism will improve our performance in this project because of our heterogeneous construction of graph and the nature of text analysis task.
        </p>
        <h3>
            <a id="support-or-contact" class="anchor" href="#support-or-contact" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>References
        </h3>
        <p>
            <ol>
                <li>
                    T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119, 2013.
                </li>
                <li>
                    Jian Tang, Meng Qu, and Qiaozhu Mei. Pte: Predictive text embedding through large-scale heterogeneous textnetworks. InProceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and DataMining, KDD ’15, pages 1165–1174, New York, NY, USA, 2015. ACM.
                </li>
                <li>
                    【此处需要修改编号！introduction用的我先放到了1和2了】Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.arXiv preprintarXiv:1609.02907, 2016.
                </li>
                <li>
                    Liang Yao, Chengsheng Mao, and Yuan Luo. Graph convolutional networks for text classification. InProceedings ofthe AAAI Conference on Artificial Intelligence, volume 33, pages 7370–7377, 2019.
                </li>

            </ol>
        </p>

        <footer class="site-footer">
            <span class="site-footer-owner"><a href="https://github.com/nathanxli/PTEexperiments">Pte: Predictive text embedding through large-scale heterogeneous text networks.</a> is maintained by <a href="https://github.com/binghesam">Bing He</a>, <a href="https://github.com/HUILIHUANG413">Huili Huang</a>, <a href="https://career.lixingchi.com">Xingchi Li</a>, <a href="https://github.com/HyperCubic">Zuoxin Tang</a>, <a href="https://github.com/JingfengYang">Jingfeng Yang</a>.</span>

            <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
        </footer>

    </section>

</body>

<script type="text/javascript" src="javascripts/word-label.js"></script>
<script type="text/javascript" src="javascripts/word-label-mr.js"></script>
<!-- <script type="text/javascript" src="javascripts/mr-line.js"></script> -->
<script type="text/javascript" src="javascripts/mult-line.js"></script>
<script type="text/javascript" src="javascripts/scatter-mr.js"></script>
<script type="text/javascript" src="javascripts/scatter-dblp.js"></script>

<!-- <script type="text/javascript">
		var width = 960,
	height = 500;

var color = d3.scaleOrdinal(d3.schemeCategory10);

// create force layout
var force = d3.forceSimulation().charge(-120).linkDistance(30).size([width, height]).distance(200);
var svg = d3.select("#bipartite").append("svg").attr("width", width).attr("height", height).append('svg:g').call(d3.behavior.zoom().on("zoom", redraw));

function redraw() {
	svg.attr("transform", "translate(" + d3.event.translate + ")" + " scale(" + d3.event.scale + ")");
	node.attr("transform", function(d) {
		return "translate(" + d.x + "," + d.y + ")";
	});
}

d3.json("bip.json", function(error, graph) {

	console.log("graph.nodes", graph.nodes)
	console.log("graph.links", graph.links)

	// start force layout
	force.nodes(graph.nodes).links(graph.links).start();

	var link = svg.selectAll(".link").data(graph.links).enter().append("line").attr("class", "link")
	.style("stroke-width", function(d) {
		return Math.sqrt(d.value);
	});

	var node=svg.selectAll('g.node').data(graph.nodes).enter().append('svg:g').attr('class','node').call(force.drag)
	var circles=node.append('circle').attr("r", 5).style("fill", function(d) {
		return color(d.group);
	});
	var texts=node.append('svg:text')
	.attr("dx", 12)
      .attr("dy", ".35em")
	.text(function(d){return d.name});
	node.append("title").text(function(d) {
		return d.name;
	});


	force.on("tick", function() {
		link.attr("x1", function(d) {
			return d.source.x;
		}).attr("y1", function(d) {
			return d.source.y;
		}).attr("x2", function(d) {
			return d.target.x;
		}).attr("y2", function(d) {
			return d.target.y;
		});

		node.attr("transform", function(d) {
			var x=d.x,
				y=d.y;
			if (x>300 && d.group=='tuple'){
				x=300
				d.x=x
			}
			if (x<500 && d.group=='pattern'){
				x=500
				d.x=x
			}

			return "translate(" + x + "," + y + ")";
		});


	});
});

	</script> -->

</html>
